{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrn\n",
    "import flax.linen as nn\n",
    "from flax import nnx\n",
    "from jax import grad, jit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d key array",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m gaussian_sampler = jrn.normal\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m key,subkey = jrn.key(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work2/Sebas/conda_envs/PDPO_jax/lib/python3.13/site-packages/jax/_src/prng.py:274\u001b[39m, in \u001b[36mPRNGKeyArray.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[PRNGKeyArray]:\n\u001b[32m    273\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_scalar():\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33miteration over a 0-d key array\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    275\u001b[39m   \u001b[38;5;66;03m# TODO(frostig): we may want to avoid iteration by slicing because\u001b[39;00m\n\u001b[32m    276\u001b[39m   \u001b[38;5;66;03m# a very common use of iteration is `k1, k2 = split(key)`, and\u001b[39;00m\n\u001b[32m    277\u001b[39m   \u001b[38;5;66;03m# slicing/indexing may be trickier to track for linearity checking\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    282\u001b[39m   \u001b[38;5;66;03m# Whatever we do, we'll want to do it by overriding\u001b[39;00m\n\u001b[32m    283\u001b[39m   \u001b[38;5;66;03m# ShapedArray._iter when the element type is KeyTy...\u001b[39;00m\n\u001b[32m    284\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m (PRNGKeyArray(\u001b[38;5;28mself\u001b[39m._impl, k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m._base_array))\n",
      "\u001b[31mTypeError\u001b[39m: iteration over a 0-d key array"
     ]
    }
   ],
   "source": [
    "gaussian_sampler = jrn.normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple MLP\n",
    "\n",
    "class SimpleMLP(nnx.Module):\n",
    "    def __init__(self, hidden_dim: int = 64, output_dim: int = 1, rngs: nnx.Rngs = None):\n",
    "        self.hidden = nnx.Linear(10, hidden_dim, rngs=rngs)  # input_dim=10\n",
    "        self.output = nnx.Linear(hidden_dim, output_dim, rngs=rngs)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = nnx.relu(self.hidden(x))\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha model structure:\n",
      "Hidden weight shape: (10, 64)\n",
      "Hidden bias shape: (64,)\n",
      "Output weight shape: (64, 1)\n",
      "Output shapes: (5, 1), (5, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "rngs = nnx.Rngs(42)\n",
    "model_alpha = SimpleMLP(rngs=rngs)\n",
    "model_beta = SimpleMLP(rngs=rngs.fork())\n",
    "\n",
    "# Test input\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (5, 10))\n",
    "\n",
    "print(\"Alpha model structure:\")\n",
    "print(f\"Hidden weight shape: {model_alpha.hidden.kernel.shape}\")\n",
    "print(f\"Hidden bias shape: {model_alpha.hidden.bias.shape}\")\n",
    "print(f\"Output weight shape: {model_alpha.output.kernel.shape}\")\n",
    "\n",
    "# Test forward pass\n",
    "out_alpha = model_alpha(x)\n",
    "out_beta = model_beta(x)\n",
    "print(f\"Output shapes: {out_alpha.shape}, {out_beta.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(model_alpha,nnx.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_models(model_alpha, model_beta, weight=0.5):\n",
    "    \"\"\"Create interpolated model - this is your spline evaluation!\"\"\"\n",
    "    # Get model states (parameters)\n",
    "    state_alpha = nnx.state(model_alpha)\n",
    "    state_beta = nnx.state(model_beta)\n",
    "    \n",
    "    # Interpolate parameters\n",
    "    interpolated_state = jax.tree.map(\n",
    "        lambda a, b: weight * a + (1 - weight) * b, \n",
    "        state_alpha, state_beta\n",
    "    )   \n",
    "    interpolate_model = nnx.clone(model_alpha)\n",
    "    nnx.update(interpolate_model,interpolated_state)\n",
    "    return interpolate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated output shape: (5, 1)\n",
      "Interpolated vs alpha: 0.402743\n"
     ]
    }
   ],
   "source": [
    "# Test interpolation\n",
    "model_gamma = interpolate_models(model_alpha, model_beta, weight=0.3)\n",
    "# model_gamma = SimpleMLP(rngs = nnx.Rngs(0))\n",
    "# nnx.update(model_gamma,parameters_gamma)\n",
    "out_gamma = model_gamma(x)\n",
    "print(f\"Interpolated output shape: {out_gamma.shape}\")\n",
    "print(f\"Interpolated vs alpha: {jnp.mean(jnp.abs(out_gamma - out_alpha)):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing gradients...\n",
      "Gradient for alpha model:\n",
      "Hidden kernel grad norm: 0.495711\n",
      "Hidden bias grad norm: 0.117731\n",
      "Output kernel grad norm: 0.926063\n",
      "\n",
      "Gradient for beta model:\n",
      "Hidden kernel grad norm: 0.495711\n",
      "Output kernel grad norm: 0.926063\n"
     ]
    }
   ],
   "source": [
    "# Test gradients through parameter interpolation\n",
    "def loss_fn(model_alpha, model_beta, x):\n",
    "    # Interpolate models\n",
    "    interpolated_model = interpolate_models(model_alpha,model_beta,)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = interpolated_model(x)\n",
    "    \n",
    "    # Loss: absolute value as requested\n",
    "    return jnp.mean(jnp.abs(output))\n",
    "\n",
    "# NNX makes gradient computation clean!\n",
    "loss_grad_fn = nnx.grad(loss_fn, argnums=(0, 1))\n",
    "\n",
    "print(\"Computing gradients...\")\n",
    "grads_alpha, grads_beta = loss_grad_fn(model_alpha, model_beta, x)\n",
    "\n",
    "print(\"Gradient for alpha model:\")\n",
    "print(f\"Hidden kernel grad norm: {jnp.linalg.norm(grads_alpha.hidden.kernel):.6f}\")\n",
    "print(f\"Hidden bias grad norm: {jnp.linalg.norm(grads_alpha.hidden.bias):.6f}\")\n",
    "print(f\"Output kernel grad norm: {jnp.linalg.norm(grads_alpha.output.kernel):.6f}\")\n",
    "\n",
    "print(\"\\nGradient for beta model:\")\n",
    "print(f\"Hidden kernel grad norm: {jnp.linalg.norm(grads_beta.hidden.kernel):.6f}\")\n",
    "print(f\"Output kernel grad norm: {jnp.linalg.norm(grads_beta.output.kernel):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nnx.Module):\n",
    "    def __init__(self, input_dim: int = 1, width: int = 10, num_layers: int = 1, \n",
    "                 output_dim: int = 1, activation=nnx.relu, rngs: nnx.Rngs = None):\n",
    "        self.input_dim = input_dim\n",
    "        self.width = width  # Fixed typo: widht -> width\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize layers\n",
    "        self.init_layer = nnx.Linear(input_dim, width, rngs=rngs)\n",
    "        \n",
    "        # Use a list instead of dict - NNX tracks lists of modules properly\n",
    "        self.layers = [nnx.Linear(width, width, rngs=rngs) for i in range(num_layers)]\n",
    "        \n",
    "        self.output_layer = nnx.Linear(width, output_dim, rngs=rngs)\n",
    "    \n",
    "    def __call__(self, x):  # Fixed: **call** -> __call__\n",
    "        x = self.init_layer(x)\n",
    "        \n",
    "        # Residual connections\n",
    "        for layer in self.layers:  # Now iterates over actual layer objects\n",
    "            residual = x\n",
    "            x = self.activation(layer(x)) + residual  # ResNet skip connection\n",
    "            \n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test parameter combination\n",
    "alpha = ResNet(input_dim=10, width=10, num_layers=1, output_dim=1, activation=nnx.relu, rngs=nnx.Rngs(0))\n",
    "beta = ResNet(input_dim=10, width=10, num_layers=1, output_dim=1, activation=nnx.relu, rngs=nnx.Rngs(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gradients of linear combination \n",
    "\n",
    "\n",
    "\n",
    "# Test gradients through parameter interpolation\n",
    "def loss_fn(model_alpha, model_beta, x):\n",
    "    # Interpolate models\n",
    "    model_combined = interpolate_models(model_alpha, model_beta, weight=0.4)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model_combined(x)\n",
    "    \n",
    "    # Loss: absolute value as requested\n",
    "    return jnp.mean(jnp.abs(output))\n",
    "\n",
    "# NNX makes gradient computation clean!\n",
    "loss_grad_fn = nnx.grad(loss_fn, argnums=(0, 1))\n",
    "grads_alpha, grads_beta = loss_grad_fn(alpha, beta, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# Ref: https://docs.jaxstack.ai/en/latest/digits_diffusion_model.html\n",
    "\n",
    "class UNet(nnx.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 features: int,\n",
    "                 time_emb_dim: int = 128,\n",
    "                 *,\n",
    "                 rngs: nnx.Rngs):\n",
    "        \"\"\"\n",
    "        Initialize the U-Net architecture with time embedding.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "\n",
    "        # Time embedding layers for diffusion timestep conditioning.\n",
    "        self.time_mlp_1 = nnx.Linear(in_features=time_emb_dim, out_features=time_emb_dim, rngs=rngs)\n",
    "        self.time_mlp_2 = nnx.Linear(in_features=time_emb_dim, out_features=time_emb_dim, rngs=rngs)\n",
    "\n",
    "        # Time projection layers for different scales.\n",
    "        self.time_proj1 = nnx.Linear(in_features=time_emb_dim, out_features=features, rngs=rngs)\n",
    "        self.time_proj2 = nnx.Linear(in_features=time_emb_dim, out_features=features * 2, rngs=rngs)\n",
    "        self.time_proj3 = nnx.Linear(in_features=time_emb_dim, out_features=features * 4, rngs=rngs)\n",
    "        self.time_proj4 = nnx.Linear(in_features=time_emb_dim, out_features=features * 8, rngs=rngs)\n",
    "\n",
    "        # The encoder path.\n",
    "        self.down_conv1 = self._create_residual_block(in_channels, features, rngs)\n",
    "        self.down_conv2 = self._create_residual_block(features, features * 2, rngs)\n",
    "        self.down_conv3 = self._create_residual_block(features * 2, features * 4, rngs)\n",
    "        self.down_conv4 = self._create_residual_block(features * 4, features * 8, rngs)\n",
    "\n",
    "        # Multi-head self-attention blocks.\n",
    "        self.attention1 = self._create_attention_block(features * 4, rngs)\n",
    "        self.attention2 = self._create_attention_block(features * 8, rngs)\n",
    "\n",
    "        # The bridge connecting the encoder and the decoder.\n",
    "        self.bridge_down = self._create_residual_block(features * 8, features * 16, rngs)\n",
    "        self.bridge_attention = self._create_attention_block(features * 16, rngs)\n",
    "        self.bridge_up = self._create_residual_block(features * 16, features * 16, rngs)\n",
    "\n",
    "        # Decoder path with skip connections.\n",
    "        self.up_conv4 = self._create_residual_block(features * 24, features * 8, rngs)\n",
    "        self.up_conv3 = self._create_residual_block(features * 12, features * 4, rngs)\n",
    "        self.up_conv2 = self._create_residual_block(features * 6, features * 2, rngs)\n",
    "        self.up_conv1 = self._create_residual_block(features * 3, features, rngs)\n",
    "\n",
    "        # Output layers.\n",
    "        self.final_norm = nnx.LayerNorm(features, rngs=rngs)\n",
    "        self.final_conv = nnx.Conv(in_features=features,\n",
    "                                 out_features=out_channels,\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 strides=(1, 1),\n",
    "                                 padding=((1, 1), (1, 1)),\n",
    "                                 rngs=rngs)\n",
    "\n",
    "    def _create_attention_block(self, channels: int, rngs: nnx.Rngs) -> Callable:\n",
    "        \"\"\"Creates a self-attention block with learned query, key, value projections.\n",
    "\n",
    "        Args:\n",
    "            channels (int): The number of channels in the input feature maps.\n",
    "            rngs (flax.nnx.Rngs): A set of named `flax.nnx.RngStream` objects that generate a stream of JAX pseudo-random number generator (PRNG) keys.\n",
    "\n",
    "        Returns:\n",
    "            Callable: A function representing a forward pass through the attention block.\n",
    "\n",
    "        \"\"\"\n",
    "        query_proj = nnx.Linear(in_features=channels, out_features=channels, rngs=rngs)\n",
    "        key_proj = nnx.Linear(in_features=channels, out_features=channels, rngs=rngs)\n",
    "        value_proj = nnx.Linear(in_features=channels, out_features=channels, rngs=rngs)\n",
    "\n",
    "        def forward(x: jax.Array) -> jax.Array:\n",
    "            \"\"\"Applies self-attention to the input.\n",
    "\n",
    "            Args:\n",
    "                x (jax.Array): The input tensor with the shape `[batch, height, width, channels]` (or `B, H, W, C`).\n",
    "\n",
    "            Returns:\n",
    "                jax.Array: The output tensor after applying self-attention.\n",
    "            \"\"\"\n",
    "\n",
    "            # Shape: batch, height, width, channels.\n",
    "            B, H, W, C = x.shape\n",
    "            scale = jnp.sqrt(C).astype(x.dtype)\n",
    "\n",
    "            # Project the input into query, key, value projections.\n",
    "            q = query_proj(x)\n",
    "            k = key_proj(x)\n",
    "            v = value_proj(x)\n",
    "\n",
    "            # Reshape for the attention computation.\n",
    "            q = q.reshape(B, H * W, C)\n",
    "            k = k.reshape(B, H * W, C)\n",
    "            v = v.reshape(B, H * W, C)\n",
    "\n",
    "            # Compute the scaled dot-product attention.\n",
    "            attention = jnp.einsum('bic,bjc->bij', q, k) / scale  # Scaled dot-product.\n",
    "            attention = jax.nn.softmax(attention, axis=-1)  # Softmax.\n",
    "\n",
    "            # The output tensor.\n",
    "            out = jnp.einsum('bij,bjc->bic', attention, v)\n",
    "            out = out.reshape(B, H, W, C)\n",
    "\n",
    "            return x + out  # A ResNet-style residual connection.\n",
    "\n",
    "        return forward\n",
    "\n",
    "    def _create_residual_block(self,\n",
    "                              in_channels: int,\n",
    "                              out_channels: int,\n",
    "                              rngs: nnx.Rngs) -> Callable:\n",
    "        \"\"\"Creates a residual block with two convolutions and normalization.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            rngs (flax.nnx.Rngs): A set of named `flax.nnx.RngStream` objects that generate a stream of JAX PRNG keys.\n",
    "\n",
    "        Returns:\n",
    "            Callable: A function that represents the forward pass through the residual block.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convolutional layers with layer normalization.\n",
    "        conv1 = nnx.Conv(in_features=in_channels,\n",
    "                        out_features=out_channels,\n",
    "                        kernel_size=(3, 3),\n",
    "                        strides=(1, 1),\n",
    "                        padding=((1, 1), (1, 1)),\n",
    "                        rngs=rngs)\n",
    "        norm1 = nnx.LayerNorm(out_channels, rngs=rngs)\n",
    "        conv2 = nnx.Conv(in_features=out_channels,\n",
    "                        out_features=out_channels,\n",
    "                        kernel_size=(3, 3),\n",
    "                        strides=(1, 1),\n",
    "                        padding=((1, 1), (1, 1)),\n",
    "                        rngs=rngs)\n",
    "        norm2 = nnx.LayerNorm(out_channels, rngs=rngs)\n",
    "\n",
    "        # Projection shortcut if dimensions change.\n",
    "        shortcut = nnx.Conv(in_features=in_channels,\n",
    "                            out_features=out_channels,\n",
    "                            kernel_size=(1, 1),\n",
    "                            strides=(1, 1),\n",
    "                            rngs=rngs)\n",
    "\n",
    "        # The forward pass through the residual block.\n",
    "        def forward(x: jax.Array) -> jax.Array:\n",
    "            identity = shortcut(x)\n",
    "\n",
    "            x = conv1(x)\n",
    "            x = norm1(x)\n",
    "            x = nnx.gelu(x)\n",
    "\n",
    "            x = conv2(x)\n",
    "            x = norm2(x)\n",
    "            x = nnx.gelu(x)\n",
    "\n",
    "            return x + identity\n",
    "\n",
    "        return forward\n",
    "\n",
    "    def _pos_encoding(self, t: jax.Array, dim: int) -> jax.Array:\n",
    "        \"\"\"Applies sinusoidal positional encoding for time embedding.\n",
    "\n",
    "        Args:\n",
    "            t (jax.Array): The time embedding, representing the timestep.\n",
    "            dim (int): The dimension of the output positional encoding.\n",
    "\n",
    "        Returns:\n",
    "            jax.Array: The sinusoidal positional embedding per timestep.\n",
    "\n",
    "        \"\"\"\n",
    "        # Calculate half the embedding dimension.\n",
    "        half_dim = dim // 2\n",
    "        # Compute the logarithmic scaling factor for sinusoidal frequencies.\n",
    "        emb = jnp.log(10000.0) / (half_dim - 1)\n",
    "        # Generate a range of sinusoidal frequencies.\n",
    "        emb = jnp.exp(jnp.arange(half_dim) * -emb)\n",
    "        # Create the positional encoding by multiplying time embeddings with.\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        # Concatenate sine and cosine components for richer representation.\n",
    "        emb = jnp.concatenate([jnp.sin(emb), jnp.cos(emb)], axis=1)\n",
    "        return emb\n",
    "\n",
    "    def _downsample(self, x: jax.Array) -> jax.Array:\n",
    "        \"\"\"Downsamples the input feature map with max pooling.\"\"\"\n",
    "        return nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2), padding='SAME')\n",
    "\n",
    "    def _upsample(self, x: jax.Array, target_size: int) -> jax.Array:\n",
    "        \"\"\"Upsamples the input feature map using nearest neighbor interpolation.\"\"\"\n",
    "        return jax.image.resize(x,\n",
    "                              (x.shape[0], target_size, target_size, x.shape[3]),\n",
    "                              method='nearest')\n",
    "\n",
    "    def __call__(self, x: jax.Array, t: jax.Array) -> jax.Array:\n",
    "        \"\"\"Perform the forward pass through the U-Net using time embeddings.\"\"\"\n",
    "\n",
    "        # Time embedding and projection.\n",
    "        t_emb = self._pos_encoding(t, 128) # Sinusoidal positional encoding for time.\n",
    "        t_emb = self.time_mlp_1(t_emb) # Project and activate the time embedding\n",
    "        t_emb = nnx.gelu(t_emb) # Activation function: `flax.nnx.gelu` (GeLU).\n",
    "        t_emb = self.time_mlp_2(t_emb)\n",
    "\n",
    "        # Project time embeddings for each scale.\n",
    "        # Project to the correct dimensions for each encoder block.\n",
    "        t_emb1 = self.time_proj1(t_emb)[:, None, None, :]\n",
    "        t_emb2 = self.time_proj2(t_emb)[:, None, None, :]\n",
    "        t_emb3 = self.time_proj3(t_emb)[:, None, None, :]\n",
    "        t_emb4 = self.time_proj4(t_emb)[:, None, None, :]\n",
    "\n",
    "        # The encoder path with time injection.\n",
    "        d1 = self.down_conv1(x)\n",
    "        t_emb1 = jnp.broadcast_to(t_emb1, d1.shape) # Broadcast the time embedding to match feature map shape.\n",
    "        d1 = d1 + t_emb1 # Add the time embedding to the feature map.\n",
    "\n",
    "        d2 = self.down_conv2(self._downsample(d1))\n",
    "        t_emb2 = jnp.broadcast_to(t_emb2, d2.shape)\n",
    "        d2 = d2 + t_emb2\n",
    "\n",
    "        d3 = self.down_conv3(self._downsample(d2))\n",
    "        d3 = self.attention1(d3) # Apply self-attention.\n",
    "        t_emb3 = jnp.broadcast_to(t_emb3, d3.shape)\n",
    "        d3 = d3 + t_emb3\n",
    "\n",
    "        d4 = self.down_conv4(self._downsample(d3))\n",
    "        d4 = self.attention2(d4)\n",
    "        t_emb4 = jnp.broadcast_to(t_emb4, d4.shape)\n",
    "        d4 = d4 + t_emb4\n",
    "\n",
    "        # The bridge.\n",
    "        b = self._downsample(d4)\n",
    "        b = self.bridge_down(b)\n",
    "        b = self.bridge_attention(b)\n",
    "        b = self.bridge_up(b)\n",
    "\n",
    "        # The decoder path with skip connections.\n",
    "        u4 = self.up_conv4(jnp.concatenate([self._upsample(b, d4.shape[1]), d4], axis=-1))\n",
    "        u3 = self.up_conv3(jnp.concatenate([self._upsample(u4, d3.shape[1]), d3], axis=-1))\n",
    "        u2 = self.up_conv2(jnp.concatenate([self._upsample(u3, d2.shape[1]), d2], axis=-1))\n",
    "        u1 = self.up_conv1(jnp.concatenate([self._upsample(u2, d1.shape[1]), d1], axis=-1))\n",
    "\n",
    "        # Final layers.\n",
    "        x = self.final_norm(u1)\n",
    "        x = nnx.gelu(x)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42) # PRNG seed for reproducibility.\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "features = 64   # Number of features in the U-Net.\n",
    "num_steps = 1000\n",
    "num_epochs = 5000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "beta_start = 1e-4   # The starting value for beta (noise level schedule).\n",
    "beta_end = 0.02   # The end value for beta (noise level schedule).\n",
    "\n",
    "# Initialize model components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key) # Split the JAX PRNG key for initialization.\n",
    "alpha = UNet(in_channels, out_channels, features, rngs=nnx.Rngs(default=subkey)) # Instantiate the U-Net.\n",
    "key,subkey = jax.random.split(key)\n",
    "beta = UNet(in_channels,out_channels,features,rngs=nnx.Rngs(default=subkey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = interpolate_models(alpha,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
